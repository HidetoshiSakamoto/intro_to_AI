{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    パラメータ配列中の重複する重みをひとつに集約し、\n",
    "    その重みに対応する勾配を加算する\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 重みを共有する場合\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 勾配の加算\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 転置行列として重みを共有する場合（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 負例のフォワード\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            p = self.word_p.copy()\n",
    "            target_idx = target[i]\n",
    "            p[target_idx] = 0\n",
    "            p /= p.sum()\n",
    "            negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoidの出力\n",
    "        self.t = None  # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 勾配を求め、パラメータを更新\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embeddingレイヤを使用\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * rn(V, H).astype('f')\n",
    "        W_out = 0.01 * rn(V, H).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer = Embedding(W_in)\n",
    "        self.loss_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "            self.loss_layers.append(layer)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer] + self.loss_layers\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "\n",
    "        loss = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            loss += layer.forward(h, contexts[:, i])\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            dh += layer.backward(dout)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''コンテキストとターゲットの作成\n",
    "\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/assignment2/senti_binary.train\") as f:\n",
    "    sentences = [re.sub('0|1|\\t','',t).replace('\\n',' <eos>').lower() for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(' '.join(sentences))\n",
    "vocab_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 7184 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 21 / 7184 | time 1[s] | loss 4.16\n",
      "| epoch 1 |  iter 41 / 7184 | time 2[s] | loss 4.15\n",
      "| epoch 1 |  iter 61 / 7184 | time 3[s] | loss 4.13\n",
      "| epoch 1 |  iter 81 / 7184 | time 4[s] | loss 4.07\n",
      "| epoch 1 |  iter 101 / 7184 | time 5[s] | loss 3.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-890c2f34721a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c2b88878aeb3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 共有された重みを1つに集約\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mclip_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-1d5c16e28628>\u001b[0m in \u001b[0;36mremove_duplicate\u001b[0;34m(params, grads)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m# 重みを共有する場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 勾配の加算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0mfind_flg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/assignment2/cbow_params.pkl\", 'rb') as f:\n",
    "    params = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def word_to_vector(word):\n",
    "    try:\n",
    "        return word_vecs[word_to_id[word]]\n",
    "    except KeyError:\n",
    "        return np.zeros(word_vecs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def return_input_data(type_):\n",
    "    with open(\"../../../data/assignment2/senti_binary.{}\".format(type_)) as f:\n",
    "        sentences = [re.sub('-|\\t',' ',t).replace('\\n','').lower() for t in f.readlines()]\n",
    "     \n",
    "    splitted = [sentence.split(' ') for sentence in sentences]\n",
    "    data_input = [(torch.Tensor(np.mean([word_to_vector(word) for word in arr[:-1] if not word == \"\"], axis=0)),int(arr[-1])) for arr in splitted]\n",
    "    \n",
    "    return data_input\n",
    "\n",
    "train_input = return_input_data('train')\n",
    "test_input = return_input_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hl1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.hl1a = nn.Tanh()\n",
    "        self.layer1 = [self.hl1, self.hl1a]\n",
    "        \n",
    "        self.hl2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.hl2a = nn.Tanh()\n",
    "        self.layer2 = [self.hl2, self.hl2a]\n",
    "        \n",
    "        self.ol = nn.Linear(hidden2_dim, output_dim)\n",
    "        self.ola = nn.Softmax()\n",
    "        self.layer3 = [self.ol, self.ola]\n",
    "        \n",
    "        self.layers = [self.layer1, self.layer2, self.layer3]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x\n",
    "        \n",
    "        for pa, a in self.layers:\n",
    "            \n",
    "            out = a(pa(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = word_vecs.shape[1]\n",
    "model = Model(dim_input, 100, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def return_acc(model, data):\n",
    "    model_eval = model.eval()\n",
    "    pred = []\n",
    "    Y = []\n",
    "    for i, (x,y) in enumerate(torch.utils.data.DataLoader(dataset=data)):\n",
    "        with torch.no_grad():\n",
    "            output = model_eval(x)\n",
    "        pred += [int(l.argmax()) for l in output]\n",
    "        Y += [int(l) for l in y]\n",
    "\n",
    "    return(accuracy_score(Y, pred)* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, data):\n",
    "        \n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=self.data, batch_size=128)\n",
    "        \n",
    "    def train(self, lr, ne):\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.1)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        self.costs = []\n",
    "        self.acc = {}\n",
    "        self.acc['train'] = []\n",
    "        self.acc['test'] = []\n",
    "        \n",
    "        for e in range(ne):\n",
    "            \n",
    "            print('training epoch %d / %d ...' %(e+1, ne))\n",
    "            \n",
    "            train_cost = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "\n",
    "                inputs = Variable(inputs)\n",
    "                targets = Variable(targets)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                train_cost += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "#            self.acc['train'].append(return_acc(self.model.eval(), train_input))\n",
    "#            self.acc['test'].append(return_acc(self.model.eval(),test_input))\n",
    "            self.costs.append(train_cost/len(train_input))\n",
    "            print('cost: %f' %(self.costs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1 / 500 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.005411\n",
      "training epoch 2 / 500 ...\n",
      "cost: 0.005384\n",
      "training epoch 3 / 500 ...\n",
      "cost: 0.005363\n",
      "training epoch 4 / 500 ...\n",
      "cost: 0.005346\n",
      "training epoch 5 / 500 ...\n",
      "cost: 0.005331\n",
      "training epoch 6 / 500 ...\n",
      "cost: 0.005318\n",
      "training epoch 7 / 500 ...\n",
      "cost: 0.005306\n",
      "training epoch 8 / 500 ...\n",
      "cost: 0.005294\n",
      "training epoch 9 / 500 ...\n",
      "cost: 0.005282\n",
      "training epoch 10 / 500 ...\n",
      "cost: 0.005270\n",
      "training epoch 11 / 500 ...\n",
      "cost: 0.005258\n",
      "training epoch 12 / 500 ...\n",
      "cost: 0.005246\n",
      "training epoch 13 / 500 ...\n",
      "cost: 0.005233\n",
      "training epoch 14 / 500 ...\n",
      "cost: 0.005220\n",
      "training epoch 15 / 500 ...\n",
      "cost: 0.005207\n",
      "training epoch 16 / 500 ...\n",
      "cost: 0.005193\n",
      "training epoch 17 / 500 ...\n",
      "cost: 0.005178\n",
      "training epoch 18 / 500 ...\n",
      "cost: 0.005163\n",
      "training epoch 19 / 500 ...\n",
      "cost: 0.005148\n",
      "training epoch 20 / 500 ...\n",
      "cost: 0.005132\n",
      "training epoch 21 / 500 ...\n",
      "cost: 0.005116\n",
      "training epoch 22 / 500 ...\n",
      "cost: 0.005100\n",
      "training epoch 23 / 500 ...\n",
      "cost: 0.005083\n",
      "training epoch 24 / 500 ...\n",
      "cost: 0.005066\n",
      "training epoch 25 / 500 ...\n",
      "cost: 0.005050\n",
      "training epoch 26 / 500 ...\n",
      "cost: 0.005033\n",
      "training epoch 27 / 500 ...\n",
      "cost: 0.005017\n",
      "training epoch 28 / 500 ...\n",
      "cost: 0.005001\n",
      "training epoch 29 / 500 ...\n",
      "cost: 0.004985\n",
      "training epoch 30 / 500 ...\n",
      "cost: 0.004970\n",
      "training epoch 31 / 500 ...\n",
      "cost: 0.004954\n",
      "training epoch 32 / 500 ...\n",
      "cost: 0.004940\n",
      "training epoch 33 / 500 ...\n",
      "cost: 0.004925\n",
      "training epoch 34 / 500 ...\n",
      "cost: 0.004911\n",
      "training epoch 35 / 500 ...\n",
      "cost: 0.004898\n",
      "training epoch 36 / 500 ...\n",
      "cost: 0.004885\n",
      "training epoch 37 / 500 ...\n",
      "cost: 0.004872\n",
      "training epoch 38 / 500 ...\n",
      "cost: 0.004860\n",
      "training epoch 39 / 500 ...\n",
      "cost: 0.004849\n",
      "training epoch 40 / 500 ...\n",
      "cost: 0.004838\n",
      "training epoch 41 / 500 ...\n",
      "cost: 0.004827\n",
      "training epoch 42 / 500 ...\n",
      "cost: 0.004817\n",
      "training epoch 43 / 500 ...\n",
      "cost: 0.004807\n",
      "training epoch 44 / 500 ...\n",
      "cost: 0.004797\n",
      "training epoch 45 / 500 ...\n",
      "cost: 0.004788\n",
      "training epoch 46 / 500 ...\n",
      "cost: 0.004780\n",
      "training epoch 47 / 500 ...\n",
      "cost: 0.004772\n",
      "training epoch 48 / 500 ...\n",
      "cost: 0.004764\n",
      "training epoch 49 / 500 ...\n",
      "cost: 0.004756\n",
      "training epoch 50 / 500 ...\n",
      "cost: 0.004749\n",
      "training epoch 51 / 500 ...\n",
      "cost: 0.004742\n",
      "training epoch 52 / 500 ...\n",
      "cost: 0.004735\n",
      "training epoch 53 / 500 ...\n",
      "cost: 0.004729\n",
      "training epoch 54 / 500 ...\n",
      "cost: 0.004723\n",
      "training epoch 55 / 500 ...\n",
      "cost: 0.004717\n",
      "training epoch 56 / 500 ...\n",
      "cost: 0.004712\n",
      "training epoch 57 / 500 ...\n",
      "cost: 0.004706\n",
      "training epoch 58 / 500 ...\n",
      "cost: 0.004701\n",
      "training epoch 59 / 500 ...\n",
      "cost: 0.004696\n",
      "training epoch 60 / 500 ...\n",
      "cost: 0.004692\n",
      "training epoch 61 / 500 ...\n",
      "cost: 0.004687\n",
      "training epoch 62 / 500 ...\n",
      "cost: 0.004683\n",
      "training epoch 63 / 500 ...\n",
      "cost: 0.004679\n",
      "training epoch 64 / 500 ...\n",
      "cost: 0.004675\n",
      "training epoch 65 / 500 ...\n",
      "cost: 0.004671\n",
      "training epoch 66 / 500 ...\n",
      "cost: 0.004667\n",
      "training epoch 67 / 500 ...\n",
      "cost: 0.004664\n",
      "training epoch 68 / 500 ...\n",
      "cost: 0.004660\n",
      "training epoch 69 / 500 ...\n",
      "cost: 0.004657\n",
      "training epoch 70 / 500 ...\n",
      "cost: 0.004654\n",
      "training epoch 71 / 500 ...\n",
      "cost: 0.004651\n",
      "training epoch 72 / 500 ...\n",
      "cost: 0.004648\n",
      "training epoch 73 / 500 ...\n",
      "cost: 0.004645\n",
      "training epoch 74 / 500 ...\n",
      "cost: 0.004642\n",
      "training epoch 75 / 500 ...\n",
      "cost: 0.004639\n",
      "training epoch 76 / 500 ...\n",
      "cost: 0.004637\n",
      "training epoch 77 / 500 ...\n",
      "cost: 0.004634\n",
      "training epoch 78 / 500 ...\n",
      "cost: 0.004632\n",
      "training epoch 79 / 500 ...\n",
      "cost: 0.004629\n",
      "training epoch 80 / 500 ...\n",
      "cost: 0.004627\n",
      "training epoch 81 / 500 ...\n",
      "cost: 0.004625\n",
      "training epoch 82 / 500 ...\n",
      "cost: 0.004623\n",
      "training epoch 83 / 500 ...\n",
      "cost: 0.004620\n",
      "training epoch 84 / 500 ...\n",
      "cost: 0.004618\n",
      "training epoch 85 / 500 ...\n",
      "cost: 0.004616\n",
      "training epoch 86 / 500 ...\n",
      "cost: 0.004615\n",
      "training epoch 87 / 500 ...\n",
      "cost: 0.004613\n",
      "training epoch 88 / 500 ...\n",
      "cost: 0.004611\n",
      "training epoch 89 / 500 ...\n",
      "cost: 0.004609\n",
      "training epoch 90 / 500 ...\n",
      "cost: 0.004607\n",
      "training epoch 91 / 500 ...\n",
      "cost: 0.004606\n",
      "training epoch 92 / 500 ...\n",
      "cost: 0.004604\n",
      "training epoch 93 / 500 ...\n",
      "cost: 0.004602\n",
      "training epoch 94 / 500 ...\n",
      "cost: 0.004601\n",
      "training epoch 95 / 500 ...\n",
      "cost: 0.004599\n",
      "training epoch 96 / 500 ...\n",
      "cost: 0.004598\n",
      "training epoch 97 / 500 ...\n",
      "cost: 0.004596\n",
      "training epoch 98 / 500 ...\n",
      "cost: 0.004595\n",
      "training epoch 99 / 500 ...\n",
      "cost: 0.004594\n",
      "training epoch 100 / 500 ...\n",
      "cost: 0.004592\n",
      "training epoch 101 / 500 ...\n",
      "cost: 0.004591\n",
      "training epoch 102 / 500 ...\n",
      "cost: 0.004590\n",
      "training epoch 103 / 500 ...\n",
      "cost: 0.004588\n",
      "training epoch 104 / 500 ...\n",
      "cost: 0.004587\n",
      "training epoch 105 / 500 ...\n",
      "cost: 0.004586\n",
      "training epoch 106 / 500 ...\n",
      "cost: 0.004585\n",
      "training epoch 107 / 500 ...\n",
      "cost: 0.004584\n",
      "training epoch 108 / 500 ...\n",
      "cost: 0.004582\n",
      "training epoch 109 / 500 ...\n",
      "cost: 0.004581\n",
      "training epoch 110 / 500 ...\n",
      "cost: 0.004580\n",
      "training epoch 111 / 500 ...\n",
      "cost: 0.004579\n",
      "training epoch 112 / 500 ...\n",
      "cost: 0.004578\n",
      "training epoch 113 / 500 ...\n",
      "cost: 0.004577\n",
      "training epoch 114 / 500 ...\n",
      "cost: 0.004576\n",
      "training epoch 115 / 500 ...\n",
      "cost: 0.004575\n",
      "training epoch 116 / 500 ...\n",
      "cost: 0.004574\n",
      "training epoch 117 / 500 ...\n",
      "cost: 0.004573\n",
      "training epoch 118 / 500 ...\n",
      "cost: 0.004572\n",
      "training epoch 119 / 500 ...\n",
      "cost: 0.004571\n",
      "training epoch 120 / 500 ...\n",
      "cost: 0.004570\n",
      "training epoch 121 / 500 ...\n",
      "cost: 0.004569\n",
      "training epoch 122 / 500 ...\n",
      "cost: 0.004569\n",
      "training epoch 123 / 500 ...\n",
      "cost: 0.004568\n",
      "training epoch 124 / 500 ...\n",
      "cost: 0.004567\n",
      "training epoch 125 / 500 ...\n",
      "cost: 0.004566\n",
      "training epoch 126 / 500 ...\n",
      "cost: 0.004565\n",
      "training epoch 127 / 500 ...\n",
      "cost: 0.004564\n",
      "training epoch 128 / 500 ...\n",
      "cost: 0.004564\n",
      "training epoch 129 / 500 ...\n",
      "cost: 0.004563\n",
      "training epoch 130 / 500 ...\n",
      "cost: 0.004562\n",
      "training epoch 131 / 500 ...\n",
      "cost: 0.004561\n",
      "training epoch 132 / 500 ...\n",
      "cost: 0.004561\n",
      "training epoch 133 / 500 ...\n",
      "cost: 0.004560\n",
      "training epoch 134 / 500 ...\n",
      "cost: 0.004559\n",
      "training epoch 135 / 500 ...\n",
      "cost: 0.004559\n",
      "training epoch 136 / 500 ...\n",
      "cost: 0.004558\n",
      "training epoch 137 / 500 ...\n",
      "cost: 0.004557\n",
      "training epoch 138 / 500 ...\n",
      "cost: 0.004556\n",
      "training epoch 139 / 500 ...\n",
      "cost: 0.004556\n",
      "training epoch 140 / 500 ...\n",
      "cost: 0.004555\n",
      "training epoch 141 / 500 ...\n",
      "cost: 0.004555\n",
      "training epoch 142 / 500 ...\n",
      "cost: 0.004554\n",
      "training epoch 143 / 500 ...\n",
      "cost: 0.004553\n",
      "training epoch 144 / 500 ...\n",
      "cost: 0.004553\n",
      "training epoch 145 / 500 ...\n",
      "cost: 0.004552\n",
      "training epoch 146 / 500 ...\n",
      "cost: 0.004552\n",
      "training epoch 147 / 500 ...\n",
      "cost: 0.004551\n",
      "training epoch 148 / 500 ...\n",
      "cost: 0.004550\n",
      "training epoch 149 / 500 ...\n",
      "cost: 0.004550\n",
      "training epoch 150 / 500 ...\n",
      "cost: 0.004549\n",
      "training epoch 151 / 500 ...\n",
      "cost: 0.004549\n",
      "training epoch 152 / 500 ...\n",
      "cost: 0.004548\n",
      "training epoch 153 / 500 ...\n",
      "cost: 0.004548\n",
      "training epoch 154 / 500 ...\n",
      "cost: 0.004547\n",
      "training epoch 155 / 500 ...\n",
      "cost: 0.004547\n",
      "training epoch 156 / 500 ...\n",
      "cost: 0.004546\n",
      "training epoch 157 / 500 ...\n",
      "cost: 0.004546\n",
      "training epoch 158 / 500 ...\n",
      "cost: 0.004545\n",
      "training epoch 159 / 500 ...\n",
      "cost: 0.004545\n",
      "training epoch 160 / 500 ...\n",
      "cost: 0.004544\n",
      "training epoch 161 / 500 ...\n",
      "cost: 0.004544\n",
      "training epoch 162 / 500 ...\n",
      "cost: 0.004543\n",
      "training epoch 163 / 500 ...\n",
      "cost: 0.004543\n",
      "training epoch 164 / 500 ...\n",
      "cost: 0.004542\n",
      "training epoch 165 / 500 ...\n",
      "cost: 0.004542\n",
      "training epoch 166 / 500 ...\n",
      "cost: 0.004541\n",
      "training epoch 167 / 500 ...\n",
      "cost: 0.004541\n",
      "training epoch 168 / 500 ...\n",
      "cost: 0.004541\n",
      "training epoch 169 / 500 ...\n",
      "cost: 0.004540\n",
      "training epoch 170 / 500 ...\n",
      "cost: 0.004540\n",
      "training epoch 171 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 172 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 173 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 174 / 500 ...\n",
      "cost: 0.004538\n",
      "training epoch 175 / 500 ...\n",
      "cost: 0.004538\n",
      "training epoch 176 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 177 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 178 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 179 / 500 ...\n",
      "cost: 0.004536\n",
      "training epoch 180 / 500 ...\n",
      "cost: 0.004536\n",
      "training epoch 181 / 500 ...\n",
      "cost: 0.004536\n",
      "training epoch 182 / 500 ...\n",
      "cost: 0.004535\n",
      "training epoch 183 / 500 ...\n",
      "cost: 0.004535\n",
      "training epoch 184 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 185 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 186 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 187 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 188 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 189 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 190 / 500 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.004532\n",
      "training epoch 191 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 192 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 193 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 194 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 195 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 196 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 197 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 198 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 199 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 200 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 201 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 202 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 203 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 204 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 205 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 206 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 207 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 208 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 209 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 210 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 211 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 212 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 213 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 214 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 215 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 216 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 217 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 218 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 219 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 220 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 221 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 222 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 223 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 224 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 225 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 226 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 227 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 228 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 229 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 230 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 231 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 232 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 233 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 234 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 235 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 236 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 237 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 238 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 239 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 240 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 241 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 242 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 243 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 244 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 245 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 246 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 247 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 248 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 249 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 250 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 251 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 252 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 253 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 254 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 255 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 256 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 257 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 258 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 259 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 260 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 261 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 262 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 263 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 264 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 265 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 266 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 267 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 268 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 269 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 270 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 271 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 272 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 273 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 274 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 275 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 276 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 277 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 278 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 279 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 280 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 281 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 282 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 283 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 284 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 285 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 286 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 287 / 500 ...\n",
      "cost: 0.004514\n",
      "training epoch 288 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 289 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 290 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 291 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 292 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 293 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 294 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 295 / 500 ...\n",
      "cost: 0.004513\n",
      "training epoch 296 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 297 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 298 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 299 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 300 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 301 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 302 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 303 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 304 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 305 / 500 ...\n",
      "cost: 0.004512\n",
      "training epoch 306 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 307 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 308 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 309 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 310 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 311 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 312 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 313 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 314 / 500 ...\n",
      "cost: 0.004511\n",
      "training epoch 315 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 316 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 317 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 318 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 319 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 320 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 321 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 322 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 323 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 324 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 325 / 500 ...\n",
      "cost: 0.004510\n",
      "training epoch 326 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 327 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 328 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 329 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 330 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 331 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 332 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 333 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 334 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 335 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 336 / 500 ...\n",
      "cost: 0.004509\n",
      "training epoch 337 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 338 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 339 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 340 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 341 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 342 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 343 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 344 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 345 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 346 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 347 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 348 / 500 ...\n",
      "cost: 0.004508\n",
      "training epoch 349 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 350 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 351 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 352 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 353 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 354 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 355 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 356 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 357 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 358 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 359 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 360 / 500 ...\n",
      "cost: 0.004507\n",
      "training epoch 361 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 362 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 363 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 364 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 365 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 366 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 367 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 368 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 369 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 370 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 371 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 372 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 373 / 500 ...\n",
      "cost: 0.004506\n",
      "training epoch 374 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 375 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 376 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 377 / 500 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.004505\n",
      "training epoch 378 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 379 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 380 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 381 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 382 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 383 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 384 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 385 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 386 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 387 / 500 ...\n",
      "cost: 0.004505\n",
      "training epoch 388 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 389 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 390 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 391 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 392 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 393 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 394 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 395 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 396 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 397 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 398 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 399 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 400 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 401 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 402 / 500 ...\n",
      "cost: 0.004504\n",
      "training epoch 403 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 404 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 405 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 406 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 407 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 408 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 409 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 410 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 411 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 412 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 413 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 414 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 415 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 416 / 500 ...\n",
      "cost: 0.004503\n",
      "training epoch 417 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 418 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 419 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 420 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 421 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 422 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 423 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 424 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 425 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 426 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 427 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 428 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 429 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 430 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 431 / 500 ...\n",
      "cost: 0.004502\n",
      "training epoch 432 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 433 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 434 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 435 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 436 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 437 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 438 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 439 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 440 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 441 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 442 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 443 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 444 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 445 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 446 / 500 ...\n",
      "cost: 0.004501\n",
      "training epoch 447 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 448 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 449 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 450 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 451 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 452 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 453 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 454 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 455 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 456 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 457 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 458 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 459 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 460 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 461 / 500 ...\n",
      "cost: 0.004500\n",
      "training epoch 462 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 463 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 464 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 465 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 466 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 467 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 468 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 469 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 470 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 471 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 472 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 473 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 474 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 475 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 476 / 500 ...\n",
      "cost: 0.004499\n",
      "training epoch 477 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 478 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 479 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 480 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 481 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 482 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 483 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 484 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 485 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 486 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 487 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 488 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 489 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 490 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 491 / 500 ...\n",
      "cost: 0.004498\n",
      "training epoch 492 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 493 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 494 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 495 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 496 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 497 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 498 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 499 / 500 ...\n",
      "cost: 0.004497\n",
      "training epoch 500 / 500 ...\n",
      "cost: 0.004497\n"
     ]
    }
   ],
   "source": [
    "trainer.train(0.001, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 72.108\n",
      "Test acc: 71.499\n"
     ]
    }
   ],
   "source": [
    "train_acc = return_acc(trainer.model, train_input)\n",
    "test_acc = return_acc(trainer.model, test_input)\n",
    "print(\"Train acc: {:.3f}\\nTest acc: {:.3f}\".format(train_acc, test_acc))\n",
    "with open(\"../results/assignment2_part2_results.txt\", 'w') as f:\n",
    "    f.write(\"{}\\n{}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(trainer.costs)), trainer.costs)\n",
    "plt.savefig('../plots/assignment2_part2_plots_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "path = \"../trained_models/part2_state.chkpt\"\n",
    "torch.save(trainer, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
