{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/assignment2/cbow_params.pkl\", 'rb') as f:\n",
    "    params = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def word_to_vector(word):\n",
    "    try:\n",
    "        return word_vecs[word_to_id[word]]\n",
    "    except KeyError:\n",
    "        return np.zeros(word_vecs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def return_input_data(type_):\n",
    "    with open(\"../../data/assignment2/senti_binary.{}\".format(type_)) as f:\n",
    "        sentences = [re.sub('-|\\t',' ',t).replace('\\n','').lower() for t in f.readlines()]\n",
    "     \n",
    "    splitted = [sentence.split(' ') for sentence in sentences]\n",
    "    data_input = [(torch.Tensor(np.mean([word_to_vector(word) for word in arr[:-1] if not word == \"\"], axis=0)),int(arr[-1])) for arr in splitted]\n",
    "    \n",
    "    return data_input\n",
    "\n",
    "train_input = return_input_data('train')\n",
    "test_input = return_input_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hl1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.hl1a = nn.Tanh()\n",
    "        self.layer1 = [self.hl1, self.hl1a]\n",
    "        \n",
    "        self.hl2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.hl2a = nn.Tanh()\n",
    "        self.layer2 = [self.hl2, self.hl2a]\n",
    "        \n",
    "        self.ol = nn.Linear(hidden2_dim, output_dim)\n",
    "        self.ola = nn.Softmax()\n",
    "        self.layer3 = [self.ol, self.ola]\n",
    "        \n",
    "        self.layers = [self.layer1, self.layer2, self.layer3]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x\n",
    "        \n",
    "        for pa, a in self.layers:\n",
    "            \n",
    "            out = a(pa(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = word_vecs.shape[1]\n",
    "model = Model(dim_input, 100, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def return_acc(model, data):\n",
    "    model_eval = model.eval()\n",
    "    pred = []\n",
    "    Y = []\n",
    "    for i, (x,y) in enumerate(torch.utils.data.DataLoader(dataset=data)):\n",
    "        with torch.no_grad():\n",
    "            output = model_eval(x)\n",
    "        pred += [int(l.argmax()) for l in output]\n",
    "        Y += [int(l) for l in y]\n",
    "\n",
    "    return(accuracy_score(Y, pred)* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, data):\n",
    "        \n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=self.data, batch_size=128)\n",
    "        \n",
    "    def train(self, lr, ne):\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.1)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        self.costs = []\n",
    "        self.acc = {}\n",
    "        self.acc['train'] = []\n",
    "        self.acc['test'] = []\n",
    "        \n",
    "        for e in range(ne):\n",
    "            \n",
    "            print('training epoch %d / %d ...' %(e+1, ne))\n",
    "            \n",
    "            train_cost = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "\n",
    "                inputs = Variable(inputs)\n",
    "                targets = Variable(targets)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                train_cost += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            self.acc['train'].append(return_acc(self.model.eval(), train_input))\n",
    "            self.acc['test'].append(return_acc(self.model.eval(),test_input))\n",
    "            self.costs.append(train_cost/len(train_input))\n",
    "            print('cost: %f' %(self.costs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1 / 500 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.005250\n",
      "training epoch 2 / 500 ...\n",
      "cost: 0.005238\n",
      "training epoch 3 / 500 ...\n",
      "cost: 0.005225\n",
      "training epoch 4 / 500 ...\n",
      "cost: 0.005211\n",
      "training epoch 5 / 500 ...\n",
      "cost: 0.005196\n",
      "training epoch 6 / 500 ...\n",
      "cost: 0.005181\n",
      "training epoch 7 / 500 ...\n",
      "cost: 0.005166\n",
      "training epoch 8 / 500 ...\n",
      "cost: 0.005150\n",
      "training epoch 9 / 500 ...\n",
      "cost: 0.005134\n",
      "training epoch 10 / 500 ...\n",
      "cost: 0.005117\n",
      "training epoch 11 / 500 ...\n",
      "cost: 0.005100\n",
      "training epoch 12 / 500 ...\n",
      "cost: 0.005082\n",
      "training epoch 13 / 500 ...\n",
      "cost: 0.005065\n",
      "training epoch 14 / 500 ...\n",
      "cost: 0.005047\n",
      "training epoch 15 / 500 ...\n",
      "cost: 0.005030\n",
      "training epoch 16 / 500 ...\n",
      "cost: 0.005013\n",
      "training epoch 17 / 500 ...\n",
      "cost: 0.004996\n",
      "training epoch 18 / 500 ...\n",
      "cost: 0.004980\n",
      "training epoch 19 / 500 ...\n",
      "cost: 0.004964\n",
      "training epoch 20 / 500 ...\n",
      "cost: 0.004948\n",
      "training epoch 21 / 500 ...\n",
      "cost: 0.004933\n",
      "training epoch 22 / 500 ...\n",
      "cost: 0.004918\n",
      "training epoch 23 / 500 ...\n",
      "cost: 0.004904\n",
      "training epoch 24 / 500 ...\n",
      "cost: 0.004890\n",
      "training epoch 25 / 500 ...\n",
      "cost: 0.004877\n",
      "training epoch 26 / 500 ...\n",
      "cost: 0.004864\n",
      "training epoch 27 / 500 ...\n",
      "cost: 0.004852\n",
      "training epoch 28 / 500 ...\n",
      "cost: 0.004840\n",
      "training epoch 29 / 500 ...\n",
      "cost: 0.004829\n",
      "training epoch 30 / 500 ...\n",
      "cost: 0.004818\n",
      "training epoch 31 / 500 ...\n",
      "cost: 0.004808\n",
      "training epoch 32 / 500 ...\n",
      "cost: 0.004798\n",
      "training epoch 33 / 500 ...\n",
      "cost: 0.004789\n",
      "training epoch 34 / 500 ...\n",
      "cost: 0.004780\n",
      "training epoch 35 / 500 ...\n",
      "cost: 0.004771\n",
      "training epoch 36 / 500 ...\n",
      "cost: 0.004763\n",
      "training epoch 37 / 500 ...\n",
      "cost: 0.004755\n",
      "training epoch 38 / 500 ...\n",
      "cost: 0.004748\n",
      "training epoch 39 / 500 ...\n",
      "cost: 0.004741\n",
      "training epoch 40 / 500 ...\n",
      "cost: 0.004734\n",
      "training epoch 41 / 500 ...\n",
      "cost: 0.004727\n",
      "training epoch 42 / 500 ...\n",
      "cost: 0.004721\n",
      "training epoch 43 / 500 ...\n",
      "cost: 0.004715\n",
      "training epoch 44 / 500 ...\n",
      "cost: 0.004709\n",
      "training epoch 45 / 500 ...\n",
      "cost: 0.004704\n",
      "training epoch 46 / 500 ...\n",
      "cost: 0.004699\n",
      "training epoch 47 / 500 ...\n",
      "cost: 0.004693\n",
      "training epoch 48 / 500 ...\n",
      "cost: 0.004689\n",
      "training epoch 49 / 500 ...\n",
      "cost: 0.004684\n",
      "training epoch 50 / 500 ...\n",
      "cost: 0.004680\n",
      "training epoch 51 / 500 ...\n",
      "cost: 0.004675\n",
      "training epoch 52 / 500 ...\n",
      "cost: 0.004671\n",
      "training epoch 53 / 500 ...\n",
      "cost: 0.004667\n",
      "training epoch 54 / 500 ...\n",
      "cost: 0.004664\n",
      "training epoch 55 / 500 ...\n",
      "cost: 0.004660\n",
      "training epoch 56 / 500 ...\n",
      "cost: 0.004657\n",
      "training epoch 57 / 500 ...\n",
      "cost: 0.004653\n",
      "training epoch 58 / 500 ...\n",
      "cost: 0.004650\n",
      "training epoch 59 / 500 ...\n",
      "cost: 0.004647\n",
      "training epoch 60 / 500 ...\n",
      "cost: 0.004644\n",
      "training epoch 61 / 500 ...\n",
      "cost: 0.004641\n",
      "training epoch 62 / 500 ...\n",
      "cost: 0.004638\n",
      "training epoch 63 / 500 ...\n",
      "cost: 0.004636\n",
      "training epoch 64 / 500 ...\n",
      "cost: 0.004633\n",
      "training epoch 65 / 500 ...\n",
      "cost: 0.004631\n",
      "training epoch 66 / 500 ...\n",
      "cost: 0.004628\n",
      "training epoch 67 / 500 ...\n",
      "cost: 0.004626\n",
      "training epoch 68 / 500 ...\n",
      "cost: 0.004624\n",
      "training epoch 69 / 500 ...\n",
      "cost: 0.004621\n",
      "training epoch 70 / 500 ...\n",
      "cost: 0.004619\n",
      "training epoch 71 / 500 ...\n",
      "cost: 0.004617\n",
      "training epoch 72 / 500 ...\n",
      "cost: 0.004615\n",
      "training epoch 73 / 500 ...\n",
      "cost: 0.004613\n",
      "training epoch 74 / 500 ...\n",
      "cost: 0.004611\n",
      "training epoch 75 / 500 ...\n",
      "cost: 0.004609\n",
      "training epoch 76 / 500 ...\n",
      "cost: 0.004608\n",
      "training epoch 77 / 500 ...\n",
      "cost: 0.004606\n",
      "training epoch 78 / 500 ...\n",
      "cost: 0.004604\n",
      "training epoch 79 / 500 ...\n",
      "cost: 0.004602\n",
      "training epoch 80 / 500 ...\n",
      "cost: 0.004601\n",
      "training epoch 81 / 500 ...\n",
      "cost: 0.004599\n",
      "training epoch 82 / 500 ...\n",
      "cost: 0.004598\n",
      "training epoch 83 / 500 ...\n",
      "cost: 0.004596\n",
      "training epoch 84 / 500 ...\n",
      "cost: 0.004595\n",
      "training epoch 85 / 500 ...\n",
      "cost: 0.004593\n",
      "training epoch 86 / 500 ...\n",
      "cost: 0.004592\n",
      "training epoch 87 / 500 ...\n",
      "cost: 0.004590\n",
      "training epoch 88 / 500 ...\n",
      "cost: 0.004589\n",
      "training epoch 89 / 500 ...\n",
      "cost: 0.004588\n",
      "training epoch 90 / 500 ...\n",
      "cost: 0.004587\n",
      "training epoch 91 / 500 ...\n",
      "cost: 0.004585\n",
      "training epoch 92 / 500 ...\n",
      "cost: 0.004584\n",
      "training epoch 93 / 500 ...\n",
      "cost: 0.004583\n",
      "training epoch 94 / 500 ...\n",
      "cost: 0.004582\n",
      "training epoch 95 / 500 ...\n",
      "cost: 0.004581\n",
      "training epoch 96 / 500 ...\n",
      "cost: 0.004579\n",
      "training epoch 97 / 500 ...\n",
      "cost: 0.004578\n",
      "training epoch 98 / 500 ...\n",
      "cost: 0.004577\n",
      "training epoch 99 / 500 ...\n",
      "cost: 0.004576\n",
      "training epoch 100 / 500 ...\n",
      "cost: 0.004575\n",
      "training epoch 101 / 500 ...\n",
      "cost: 0.004574\n",
      "training epoch 102 / 500 ...\n",
      "cost: 0.004573\n",
      "training epoch 103 / 500 ...\n",
      "cost: 0.004572\n",
      "training epoch 104 / 500 ...\n",
      "cost: 0.004571\n",
      "training epoch 105 / 500 ...\n",
      "cost: 0.004570\n",
      "training epoch 106 / 500 ...\n",
      "cost: 0.004569\n",
      "training epoch 107 / 500 ...\n",
      "cost: 0.004568\n",
      "training epoch 108 / 500 ...\n",
      "cost: 0.004567\n",
      "training epoch 109 / 500 ...\n",
      "cost: 0.004567\n",
      "training epoch 110 / 500 ...\n",
      "cost: 0.004566\n",
      "training epoch 111 / 500 ...\n",
      "cost: 0.004565\n",
      "training epoch 112 / 500 ...\n",
      "cost: 0.004564\n",
      "training epoch 113 / 500 ...\n",
      "cost: 0.004563\n",
      "training epoch 114 / 500 ...\n",
      "cost: 0.004562\n",
      "training epoch 115 / 500 ...\n",
      "cost: 0.004562\n",
      "training epoch 116 / 500 ...\n",
      "cost: 0.004561\n",
      "training epoch 117 / 500 ...\n",
      "cost: 0.004560\n",
      "training epoch 118 / 500 ...\n",
      "cost: 0.004559\n",
      "training epoch 119 / 500 ...\n",
      "cost: 0.004559\n",
      "training epoch 120 / 500 ...\n",
      "cost: 0.004558\n",
      "training epoch 121 / 500 ...\n",
      "cost: 0.004557\n",
      "training epoch 122 / 500 ...\n",
      "cost: 0.004556\n",
      "training epoch 123 / 500 ...\n",
      "cost: 0.004556\n",
      "training epoch 124 / 500 ...\n",
      "cost: 0.004555\n",
      "training epoch 125 / 500 ...\n",
      "cost: 0.004554\n",
      "training epoch 126 / 500 ...\n",
      "cost: 0.004554\n",
      "training epoch 127 / 500 ...\n",
      "cost: 0.004553\n",
      "training epoch 128 / 500 ...\n",
      "cost: 0.004553\n",
      "training epoch 129 / 500 ...\n",
      "cost: 0.004552\n",
      "training epoch 130 / 500 ...\n",
      "cost: 0.004551\n",
      "training epoch 131 / 500 ...\n",
      "cost: 0.004551\n",
      "training epoch 132 / 500 ...\n",
      "cost: 0.004550\n",
      "training epoch 133 / 500 ...\n",
      "cost: 0.004549\n",
      "training epoch 134 / 500 ...\n",
      "cost: 0.004549\n",
      "training epoch 135 / 500 ...\n",
      "cost: 0.004548\n",
      "training epoch 136 / 500 ...\n",
      "cost: 0.004548\n",
      "training epoch 137 / 500 ...\n",
      "cost: 0.004547\n",
      "training epoch 138 / 500 ...\n",
      "cost: 0.004547\n",
      "training epoch 139 / 500 ...\n",
      "cost: 0.004546\n",
      "training epoch 140 / 500 ...\n",
      "cost: 0.004546\n",
      "training epoch 141 / 500 ...\n",
      "cost: 0.004545\n",
      "training epoch 142 / 500 ...\n",
      "cost: 0.004545\n",
      "training epoch 143 / 500 ...\n",
      "cost: 0.004544\n",
      "training epoch 144 / 500 ...\n",
      "cost: 0.004544\n",
      "training epoch 145 / 500 ...\n",
      "cost: 0.004543\n",
      "training epoch 146 / 500 ...\n",
      "cost: 0.004543\n",
      "training epoch 147 / 500 ...\n",
      "cost: 0.004542\n",
      "training epoch 148 / 500 ...\n",
      "cost: 0.004542\n",
      "training epoch 149 / 500 ...\n",
      "cost: 0.004541\n",
      "training epoch 150 / 500 ...\n",
      "cost: 0.004541\n",
      "training epoch 151 / 500 ...\n",
      "cost: 0.004540\n",
      "training epoch 152 / 500 ...\n",
      "cost: 0.004540\n",
      "training epoch 153 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 154 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 155 / 500 ...\n",
      "cost: 0.004539\n",
      "training epoch 156 / 500 ...\n",
      "cost: 0.004538\n",
      "training epoch 157 / 500 ...\n",
      "cost: 0.004538\n",
      "training epoch 158 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 159 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 160 / 500 ...\n",
      "cost: 0.004537\n",
      "training epoch 161 / 500 ...\n",
      "cost: 0.004536\n",
      "training epoch 162 / 500 ...\n",
      "cost: 0.004536\n",
      "training epoch 163 / 500 ...\n",
      "cost: 0.004535\n",
      "training epoch 164 / 500 ...\n",
      "cost: 0.004535\n",
      "training epoch 165 / 500 ...\n",
      "cost: 0.004535\n",
      "training epoch 166 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 167 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 168 / 500 ...\n",
      "cost: 0.004534\n",
      "training epoch 169 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 170 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 171 / 500 ...\n",
      "cost: 0.004533\n",
      "training epoch 172 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 173 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 174 / 500 ...\n",
      "cost: 0.004532\n",
      "training epoch 175 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 176 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 177 / 500 ...\n",
      "cost: 0.004531\n",
      "training epoch 178 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 179 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 180 / 500 ...\n",
      "cost: 0.004530\n",
      "training epoch 181 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 182 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 183 / 500 ...\n",
      "cost: 0.004529\n",
      "training epoch 184 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 185 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 186 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 187 / 500 ...\n",
      "cost: 0.004528\n",
      "training epoch 188 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 189 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 190 / 500 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.004527\n",
      "training epoch 191 / 500 ...\n",
      "cost: 0.004527\n",
      "training epoch 192 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 193 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 194 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 195 / 500 ...\n",
      "cost: 0.004526\n",
      "training epoch 196 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 197 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 198 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 199 / 500 ...\n",
      "cost: 0.004525\n",
      "training epoch 200 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 201 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 202 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 203 / 500 ...\n",
      "cost: 0.004524\n",
      "training epoch 204 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 205 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 206 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 207 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 208 / 500 ...\n",
      "cost: 0.004523\n",
      "training epoch 209 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 210 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 211 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 212 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 213 / 500 ...\n",
      "cost: 0.004522\n",
      "training epoch 214 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 215 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 216 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 217 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 218 / 500 ...\n",
      "cost: 0.004521\n",
      "training epoch 219 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 220 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 221 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 222 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 223 / 500 ...\n",
      "cost: 0.004520\n",
      "training epoch 224 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 225 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 226 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 227 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 228 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 229 / 500 ...\n",
      "cost: 0.004519\n",
      "training epoch 230 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 231 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 232 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 233 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 234 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 235 / 500 ...\n",
      "cost: 0.004518\n",
      "training epoch 236 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 237 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 238 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 239 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 240 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 241 / 500 ...\n",
      "cost: 0.004517\n",
      "training epoch 242 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 243 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 244 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 245 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 246 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 247 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 248 / 500 ...\n",
      "cost: 0.004516\n",
      "training epoch 249 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 250 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 251 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 252 / 500 ...\n",
      "cost: 0.004515\n",
      "training epoch 253 / 500 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-082a1832b8e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-279-5e0704f57919>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, ne)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-278-290e25c3de54>\u001b[0m in \u001b[0;36mreturn_acc\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-b2913d7c5c01>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(0.001, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(trainer.costs)), trainer.costs)\n",
    "plt.savefig('./plots/assignment2_part2_plots_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer.acc['train'])\n",
    "plt.plot(trainer.acc['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72.18963904438075"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_acc(trainer.model, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s08779/.pyenv/versions/anaconda3-4.3.1/envs/py3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71.49917627677101"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_acc(trainer.model, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./trained_models/part2_state.chkpt\"\n",
    "torch.save(trainer, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
